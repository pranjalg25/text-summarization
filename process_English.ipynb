{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObvqcYkRSfmm78RvdP32Tq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":43,"metadata":{"id":"Dt3iNXT4lOug","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702407911036,"user_tz":-330,"elapsed":701,"user":{"displayName":"Pranjal Gupta","userId":"04939919793015035806"}},"outputId":"b62bebe2-44b5-4b9c-ea99-54e04e5b57d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finished writing file content/drive/MyDrive/Colab Notebooks/Model 4/data/finished_files/test.bin\n","\n","Finished writing file content/drive/MyDrive/Colab Notebooks/Model 4/data/finished_files/val.bin\n","\n","Finished writing file content/drive/MyDrive/Colab Notebooks/Model 4/data/finished_files/train.bin\n","\n","Writing vocab file...\n","Finished writing vocab file\n","Splitting train data into chunks...\n","Splitting val data into chunks...\n","Splitting test data into chunks...\n","Saved chunked data in content/drive/MyDrive/Colab Notebooks/Model 4/data/finished_files/chunked\n"]}],"source":["from progressbar import ProgressBar\n","\n","import sys\n","import os\n","import hashlib\n","import struct\n","import subprocess\n","import collections\n","import tensorflow as tf\n","from tensorflow.core.example import example_pb2\n","import nltk\n","import pandas as pd\n","\n","#for cleaning text\n","def clean_text(text, remove_stopwords = True):\n","    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n","\n","    # Convert words to lower case\n","    text = text.lower()\n","\n","    # Replace contractions with their longer forms\n","    if True:\n","        text = text.split()\n","        new_text = []\n","        for word in text:\n","            if word in contractions:\n","                new_text.append(contractions[word])\n","            else:\n","                new_text.append(word)\n","        text = \" \".join(new_text)\n","\n","    # Format words and remove unwanted characters\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\<a href', ' ', text)\n","    text = re.sub(r'&amp;', '', text)\n","    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'\\'', ' ', text)\n","\n","    # Optionally, remove stop words\n","    if remove_stopwords:\n","        text = text.split()\n","        stops = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stops]\n","        text = \" \".join(text)\n","\n","    return text\n","\n","\n","dm_single_close_quote = u'\\u2019' # unicode\n","dm_double_close_quote = u'\\u201d'\n","END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n","\n","# We use these to separate the summary sentences in the .bin datafiles\n","SENTENCE_START = '<s>'\n","SENTENCE_END = '</s>'\n","\n","all_train_urls = \"\"\n","all_val_urls = \"\"\n","all_test_urls = \"\"\n","\n","#!unzip \"drive/MyDrive/Colab Notebooks/Model 4/data/dm_stories_tokenized.zip\"\n","#!unzip \"drive/MyDrive/Colab Notebooks/Model 4/data/cnn_stories_tokenized.zip\"\n","\n","\n","cnn_tokenized_stories_dir = \"content/drive/MyDrive/Colab Notebooks/Model 4/data/cnn_stories_tokenized\" #location of folder to tokenize text\n","dm_tokenized_stories_dir = \"content/drive/MyDrive/Colab Notebooks/Model 4/data/dm_stories_tokenized\" #not used\n","finished_files_dir = \"content/drive/MyDrive/Colab Notebooks/Model 4/data/finished_files\" #final ouput\n","chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n","\n","\n","\n","VOCAB_SIZE = 200000\n","CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n","\n","\n","def chunk_file(set_name):\n","  in_file = finished_files_dir + '/%s.bin' % set_name\n","  reader = open(in_file, \"rb\")\n","  chunk = 0\n","  finished = False\n","  while not finished:\n","    chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n","    with open(chunk_fname, 'wb') as writer:\n","      for _ in range(CHUNK_SIZE):\n","        len_bytes = reader.read(8)\n","        if not len_bytes:\n","          finished = True\n","          break\n","        str_len = struct.unpack('q', len_bytes)[0]\n","        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n","        writer.write(struct.pack('q', str_len))\n","        writer.write(struct.pack('%ds' % str_len, example_str))\n","      chunk += 1\n","\n","\n","def chunk_all():\n","  # Make a dir to hold the chunks\n","  if not os.path.isdir(chunks_dir):\n","    os.mkdir(chunks_dir)\n","  # Chunk the data\n","  for set_name in ['train', 'val', 'test']:\n","    print (\"Splitting %s data into chunks...\" % set_name)\n","    chunk_file(set_name)\n","  print (\"Saved chunked data in %s\" % chunks_dir)\n","\n","\n","def tokenize_stories(reviews, tokenized_stories_dir):\n","  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n","  progress = ProgressBar.ProgressBar(len(reviews), fmt=ProgressBar.ProgressBar.FULL)\n","\n","  for i, row in reviews.iterrows():\n","        #if i==20:\n","        #    break\n","        filename = str(i) + '.tok'\n","        with open(os.path.join(tokenized_stories_dir, filename), 'w', encoding=\"utf-8\") as temp_file:\n","            text = row[\"content\"]\n","            text = clean_text(text , remove_stopwords = True)\n","            tok = nltk.word_tokenize(text)\n","            tok.append(\"@highlight\")\n","            Summary = row[\"title\"]\n","            Summary = clean_text(Summary ,remove_stopwords = False)\n","            tok.extend(nltk.word_tokenize(Summary))\n","            list = tok.copy()\n","\n","            for i in tok:\n","                if(i=='``' or i==\"''\" ):\n","                    list.remove(i)\n","            tok_string = \"\\n\".join(str(x) for x in list)\n","            temp_file.write(tok_string)\n","\n","        progress.current += 1\n","        progress()\n","  print(\"Successfully finished tokenizing to %s .\\n\" % (tokenized_stories_dir))\n","\n","\n","def fix_missing_period(line):\n","  \"\"\"Adds a period to a line that is missing a period\"\"\"\n","  if \"@highlight\" in line: return line\n","  if line==\"\": return line\n","  if line[-1] in END_TOKENS: return line\n","  # print line[-1]\n","  return line + \" .\"\n","\n","def read_text_file(text_file):\n","  lines = []\n","  with open(text_file, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","      lines.append(line.strip())\n","  return lines\n","\n","def get_art_abs(story_file):\n","  lines = read_text_file(story_file)\n","\n","  # Lowercase everything\n","  lines = [line.lower() for line in lines]\n","\n","  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n","  lines = [fix_missing_period(line) for line in lines]\n","\n","  # Separate out article and abstract sentences\n","  article_lines = []\n","  highlights = []\n","  next_is_highlight = False\n","  for idx,line in enumerate(lines):\n","    if line == \"\":\n","      continue # empty line\n","    elif line.startswith(\"@highlight\"):\n","      next_is_highlight = True\n","    elif next_is_highlight:\n","      highlights.append(line)\n","    else:\n","      article_lines.append(line)\n","\n","  # Make article into a single string\n","  article = ' '.join(article_lines)\n","\n","  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n","  abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n","\n","  return article, abstract\n","\n","\n","def write_to_bin(file_names, out_file, makevocab=False):\n","  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n","\n","  story_fnames = [str(s)+\".tok\" for s in file_names]\n","  num_stories = len(story_fnames)\n","\n","  if makevocab:\n","    vocab_counter = collections.Counter()\n","\n","  with open(out_file, 'wb') as writer:\n","    for idx,s in enumerate(story_fnames):\n","      if idx % 1000 == 0:\n","        print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n","\n","      # Look in the tokenized story dirs to find the .story file corresponding to this url\n","      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir, s)):\n","        story_file = os.path.join(cnn_tokenized_stories_dir, s)\n","      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n","        story_file = os.path.join(dm_tokenized_stories_dir, s)\n","      else:\n","        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n","        # Check again if tokenized stories directories contain correct number of files\n","        print (\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n","        #check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n","        #check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n","        #raise Exception(\"Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither.\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n","\n","      # Get the strings to write to .bin file\n","      article, abstract = get_art_abs(story_file)\n","\n","\n","      # Write to tf.Example\n","      tf_example = example_pb2.Example()\n","      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n","      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n","      tf_example_str = tf_example.SerializeToString()\n","      str_len = len(tf_example_str)\n","      writer.write(struct.pack('q', str_len))\n","      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n","\n","\n","      # Write the vocab to file, if applicable\n","      if makevocab:\n","        art_tokens = article.split(' ')\n","        abs_tokens = abstract.split(' ')\n","        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n","        tokens = art_tokens + abs_tokens\n","        tokens = [t.strip() for t in tokens] # strip\n","        tokens = [t for t in tokens if t!=\"\"] # remove empty\n","        vocab_counter.update(tokens)\n","\n","  print (\"Finished writing file %s\\n\" % out_file)\n","\n","  # write vocab to file\n","  if makevocab:\n","    print (\"Writing vocab file...\")\n","    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n","      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n","        writer.write(word + ' ' + str(count) + '\\n')\n","    print (\"Finished writing vocab file\")\n","\n","\n","def check_num_stories(stories_dir, num_expected):\n","  num_stories = len(os.listdir(stories_dir))\n","  if num_stories != num_expected:\n","    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))\n","\n","\n","\n","\n","\n","\"\"\"\n","the requirements are , having\n","    1- csv of your data set having 2 columbs\n","        content(text) |  summary\n","        by modifying\n","        cnn_stories_dir to pointtto your main directory\n","        and then replacing \\ArabicBook00.csv\n","        with your csv\n","\n","\n","output would be\n","    1- folder (cnn_stories_tokenized) used internally here\n","    2- finished files (the folder that we would use)\n","        |--> (folder) chunks ==> (used in upload)\n","        |--> test.bin  |\n","        |--> train.bin |--> not used in upload\n","        |--> val.bin   |\n","        |--> vocab  ==> (used in upload)\n","\n","    then\n","    put both\n","      |--> (folder) chunks ==> (used in upload)\n","      |--> vocab  ==> (used in upload)\n","      in a zip and upload online\n","\"\"\"\n","\n","\n","\n","if __name__ == '__main__':\n","  #main directory\n","  cnn_stories_dir =  r\"/content/__MACOSX/cnn_stories_tokenized\"\n","\n","  # Create some new directories\n","  if not os.path.exists(cnn_tokenized_stories_dir): os.makedirs(cnn_tokenized_stories_dir)\n","  if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n","\n","  #data needed is in a csv format\n","  #containg 2 columbs (content , title)\n","  #reviews_csv =cnn_stories_dir + \"/ArabicBook00.csv\"\n","  #reviews = pd.read_csv(reviews_csv)\n","  #reviews = reviews.filter(['content', 'title'])\n","  #reviews = reviews.dropna()\n","  #reviews = reviews.reset_index(drop=True)\n","  #reviews.head()\n","\n","  # Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n","  #tokenize_stories(reviews, cnn_tokenized_stories_dir)\n","\n","  #to get the length of your dataset\n","  #num_expected_cnn_stories =reviews.shape[0]\n","\n","  #testing len = 2000\n","  #validation lenght = 2000\n","  #all_train_urls = range(0,num_expected_cnn_stories-2000)\n","  #all_val_urls = range(num_expected_cnn_stories-2000,num_expected_cnn_stories-1000)\n","  #all_test_urls = range(num_expected_cnn_stories-1000,num_expected_cnn_stories)\n","\n","  #for testing\n","  ##############all_train_urls= range(0,80)\n","  ##############all_val_urls = range(80,90)\n","  ##############all_test_urls = range(90,100)\n","\n","  # Read the tokenized stories, do a little postprocessing then write to bin files\n","  write_to_bin(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n","  write_to_bin(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n","  write_to_bin(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n","\n","  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n","  chunk_all()\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1HCW-81rLvbY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702405396695,"user_tz":-330,"elapsed":18678,"user":{"displayName":"Pranjal Gupta","userId":"04939919793015035806"}},"outputId":"53d25fbe-1af8-4388-a9cd-aa5381498d77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}